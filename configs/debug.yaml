search:
  run_name: 'TEST'
  penalty: 0.01
  use_adjuster: true
  unrolled: false
  debug_mode: true # run on a small porion of data
  gpu: 3
  use_l1_alpha: true
  l1_lambda: 0.5
  use_soft_edge: true
  alpha_selector: softmax #softmax #softmax #gumbel2k #softmax 
  warm_up: 0
  
  
  log_dir: '/home/dev/data_main/LOGS/DARTS'
  im_dir: 'arch_images'
  dataset: MNIST  # CIFAR10 / MNIST / FashionMNIST
  data_path: 'data'
  batch_size: 8 # batch size
  w_lr: 0.025 # lr for weights
  w_lr_min: 0.001 # minimum lr for weights
  w_momentum: 0.9 # momentum for weights
  w_weight_decay: 3e-4 # weight decay for weights
  w_grad_clip: 5 # gradient clipping for weights
  print_freq: 50 # print frequency
  epochs: 5 # n of training epochs
  init_channels: 16
  temperature_start: 0.001
  temp_red: 0.1
  layers: 3 # of layers 8 is default
  seed: 2 #random seed
  workers: 4 ## of workers
  alpha_lr: 3e-2 #lr for alpha
  alpha_weight_decay: 1e-3 #weight decay for alpha
  train_portion: 0.2  # portion of training data
  adjuster: 
    lr: 0.001
    CL: 1e+5
    CH: 1e+20 # set some big value if no upper bound 
    max_iter: 100
    verbose: true
    gamma: 1e-6


train:
  run_name: 'baseline_MNIST_2_CIFAR'
  log_dir: '/home/dev/data_main/LOGS/DARTS'
  dataset: 'CIFAR10'  # CIFAR10 / MNIST / FashionMNIST
  gpu: 1
  batch_size : 128 # batch size
  lr : 0.025 # lr for weights
  momentum : 0.9 # momentum
  weight_decay : 3e-4 # weight decay
  grad_clip : 5 # gradient clipping for weights
  print_freq : 200 # print frequency
  epochs : 2 # # of training epochs
  init_channels : 36
  layers : 8 # n of layers
  seed : 2 # random seed
  workers : 4 # of workers
  aux_weight : 0.4 # auxiliary loss weight
  cutout_length : 16 # cutout length
  drop_path_prob : 0.2 # drop path prob
  genotype_path:  '/home/dev/data_main/LOGS/DARTS/arch_MNIST.gen'
  data_path: 'data'